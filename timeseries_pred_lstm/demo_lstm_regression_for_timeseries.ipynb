{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo_lstm_regression_for_timeseries.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqHxU98GWcrt"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet pytorch-lightning==1.2.5 tqdm==4.59.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import rc\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "eIFG2LC6W3uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set some plotting parameters\n",
        "%matplotlib inline\n",
        "sns.set_style(style='whitegrid')\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "_nnCWWIMgCzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a time series dataset\n",
        "We want to generate a time series based on a N-ordered Markov process, or more precisely, on a differential equation encoding a big lag. This is the equivalent of a system with some short-term and some long-term dynamics, or a fast and slow poles."
      ],
      "metadata": {
        "id": "IV8bc16ogsL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURES = [f'x_{10-k}' for k in range(10)]\n",
        "u = 0.1 * np.random.randn(10_000).astype(np.float32)\n",
        "\n",
        "def generate_time_series(u) -> pd.DataFrame:\n",
        "    # Parameters here are hard-coded. For the sake of the experiment, this works fine for now.\n",
        "    X = np.zeros(10_000, dtype=np.float32)\n",
        "    \n",
        "    for k in range(10, len(X)):\n",
        "        X[k] = 0.9 * X[k-1] - 0.4 * X[k-2] + 0.4 * X[k-10] + u[k]\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "eoWhHQ2-iNt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale(X: np.ndarray, lower=-1.0, upper=1.0) -> np.ndarray:\n",
        "    # Warning: Technically, this is a tiny bit dirty here, as we normally should only rescale for\n",
        "    # values gleaned from the test-set only. However, in our little toy example, this helps have\n",
        "    # the code a bit cleaner and more readable than having to do a train-test-split on X and then\n",
        "    # rescale both separately and then create sequences from that.\n",
        "    X_std = (X - X.min()) / (X.max() - X.min())\n",
        "    return X_std * (upper - lower) + lower"
      ],
      "metadata": {
        "id": "RGqys-z7zQdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_measurements_from_timeline(X, sequence_length=10):\n",
        "    # Creates a DataFrame out of the hidden states and the measurements:\n",
        "    return pd.DataFrame(\n",
        "        [X[k:k+sequence_length+1] for k in range(len(X) - sequence_length)],\n",
        "        columns=FEATURES + ['y']\n",
        "    )"
      ],
      "metadata": {
        "id": "qxildk7lmqDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate measurements and internal states and randomly shuffle them:\n",
        "d = generate_measurements_from_timeline(rescale(generate_time_series(u)))"
      ],
      "metadata": {
        "id": "wbAogfmFrPLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that d now contains row-wise the sequences we want to input to the LSTM.\n",
        "For our little toy example, the LSTM will be effectively working in a univariate version. For a multivariate version, we would have to adjust the code such that the we would generate a matrix with M variables and N lines of measurements of those variables."
      ],
      "metadata": {
        "id": "KFW5Euy-QE1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences from those measurements in a way that Pytorch likes it ... :\n",
        "def create_sequences(d: pd.DataFrame, measurement_col: str, target_col: str, sequence_length: int) -> list:\n",
        "    sequences = []\n",
        "\n",
        "    for i in tqdm(range(len(d) - sequence_length)):\n",
        "        sequence = d.loc[i:i+sequence_length, measurement_col]\n",
        "        label = d.loc[i+sequence_length, target_col]\n",
        "        sequences.append((sequence, label))\n",
        "\n",
        "    return sequences"
      ],
      "metadata": {
        "id": "FpnHtL8c36DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = create_sequences(d, 'x_1', 'y', 10)"
      ],
      "metadata": {
        "id": "hLe77_pIQnig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOW randomly shuffle them!\n",
        "random.shuffle(sequences)"
      ],
      "metadata": {
        "id": "Al5Qkicn60_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign sequences to train and test sets:\n",
        "N_test = int(0.9 * len(sequences))\n",
        "train_sequences, test_sequences = sequences[:N_test], sequences[N_test:]"
      ],
      "metadata": {
        "id": "36PktzTd1ip0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Pytorch Dataset and Dataloader classes"
      ],
      "metadata": {
        "id": "zAgynwwA7e-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimelineDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence, target = self.sequences[idx]\n",
        "        # Return a dictionary with sequence and target as keys.\n",
        "        # As we are handling a single-input case here, we have to unsqueeze\n",
        "        # in order to add a dimension such that we have a 3D-tensor in the\n",
        "        # end, which is what the more general multi-variate case expects.\n",
        "        return dict(\n",
        "            sequence=torch.Tensor(sequence.to_numpy(dtype=np.float32)).unsqueeze(dim=1),\n",
        "            target=torch.tensor(target.astype(np.float32))\n",
        "        )"
      ],
      "metadata": {
        "id": "1R6CIb11xDN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimelineDatamodule(pl.LightningDataModule):\n",
        "    def __init__(self, train_sequences, test_sequences, batch_size=10):\n",
        "        self.train_sequences = train_sequences\n",
        "        self.test_sequences = test_sequences\n",
        "        self.batch_size = batch_size\n",
        "        self.prepare_data()\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self):\n",
        "        self.train_dataset = TimelineDataset(self.train_sequences)\n",
        "        self.test_dataset = TimelineDataset(self.test_sequences)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "        )\n",
        "  \n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=1,\n",
        "        )\n",
        "  \n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=1,\n",
        "        )"
      ],
      "metadata": {
        "id": "VV-t6IiP_zeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the data module:\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "data_module = TimelineDatamodule(train_sequences, test_sequences, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "WGPeMHrIBF6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Pytorch model"
      ],
      "metadata": {
        "id": "Y6yF1NQD7jBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimelineLSTMModule(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            batch_first=True,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        self.regressor = torch.nn.Linear(in_features=hidden_size, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten parameters for better GPU-memory-usage in distributed training\n",
        "        # (good practice, though not really needed here):\n",
        "        self.lstm.flatten_parameters()\n",
        "\n",
        "        _, (lstm_hidden, _) = self.lstm(x)\n",
        "\n",
        "        return self.regressor(lstm_hidden[-1])"
      ],
      "metadata": {
        "id": "N9twlaurpN4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimelineLSTMModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = TimelineLSTMModule(\n",
        "            input_size=input_size, hidden_size=hidden_size, \n",
        "            num_layers=num_layers, dropout=dropout\n",
        "        )\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "\n",
        "    def forward(self, x, target=None):\n",
        "        output = self.model(x)\n",
        "        loss = 0\n",
        "        if target is not None:\n",
        "            loss = self.criterion(output, target.unsqueeze(dim=1))\n",
        "        return loss, output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, output = self(batch['sequence'], batch['target'])\n",
        "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, output = self(batch['sequence'], batch['target'])\n",
        "        self.log('val_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, output = self(batch['sequence'], batch['target'])\n",
        "        self.log('test_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "MCF8vFsVp-rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TimelineLSTMModel(input_size=1, hidden_size=64, num_layers=2, dropout=0.2)"
      ],
      "metadata": {
        "id": "pqIGQ2Kgp-wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and logging"
      ],
      "metadata": {
        "id": "BJG1W1zc4amU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up tensorboard to monitor the trainings:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./lightning_logs"
      ],
      "metadata": {
        "id": "0Am7Tfz7p-2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the callbacks, logger and trainer:\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    dirpath='checkpoints',\n",
        "    verbose=True,\n",
        "    save_top_k=1,\n",
        "    filename='best_model',\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "early_stopping_callback = pl.callbacks.EarlyStopping('val_loss', patience=5, verbose=True, mode='min')\n",
        "\n",
        "logger = pl.loggers.TensorBoardLogger('lightning_logs', name='timeline_prediction')\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    callbacks=[early_stopping_callback],\n",
        "    logger=logger,\n",
        "    max_epochs=250,\n",
        "    gpus=1,\n",
        "    progress_bar_refresh_rate=30,\n",
        ")"
      ],
      "metadata": {
        "id": "oE3jJT2G3-_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model, datamodule=data_module)"
      ],
      "metadata": {
        "id": "-NuUlZ8g5USu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyse model results"
      ],
      "metadata": {
        "id": "u5gaClis8D9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = TimelineLSTMModel.load_from_checkpoint('./checkpoints/best_model.ckpt', input_size=1, hidden_size=64, num_layers=2)\n",
        "trained_model.freeze()"
      ],
      "metadata": {
        "id": "4jUarsOfQlLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TimelineDataset(test_sequences)"
      ],
      "metadata": {
        "id": "7cjak69aQupM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "targets = []\n",
        "\n",
        "for t in tqdm(test_dataset):\n",
        "    _, output = trained_model(t['sequence'].unsqueeze(dim=0))\n",
        "    predictions.append(output.item())\n",
        "    targets.append(t['target'].tolist())"
      ],
      "metadata": {
        "id": "Q-02vnBpRg3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a dataframe to compare predictions and targets:\n",
        "res = pd.DataFrame(\n",
        "    {'predictions': predictions, 'targets': targets}\n",
        ")\n",
        "\n",
        "# Order that dataframe such that the targets are ascending,\n",
        "# to get a cleaner view:\n",
        "res = res.sort_values('targets').reset_index(drop=True)"
      ],
      "metadata": {
        "id": "SLDdc4x1Skzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res.plot(y=['targets', 'predictions'])"
      ],
      "metadata": {
        "id": "Q3QIxdmklXet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that the model does not know the random-disturbance $u$, the result is as expected: The general trend is caught. All is working fine."
      ],
      "metadata": {
        "id": "bQUKDdw9lX1_"
      }
    }
  ]
}